# Introduction
Intuitions are an invaluable tool in grasping complex data science concepts. They wrap the complex concept, say neural networks, in a narrative that explains something about the nature of the concept. The narrative helps us look at the concept from a holistic angle, binding all the constituient parts of the concept (neurons, activation function, weights, biases) in one big conherent narrative. From this coherent narrative it Is easier to really grasp the concept, even when going down to the details. 

I plan to write an article that collects a great many interesting data science related intuitions. Where possible I will provide the source where I got the intuition from, but some of them I can either not remember the source, or they are my own intuitions. Any omissions in this are strictly my own. 

The idea for the article is to take some or all of the intuitions below and expand them into an article. 

# List of intuitions

- Neural networks can be see as representation building machines. They take the input data, say an image, and through multiple network layers build more and more complex representations of the input image. By training the network we find the representations that most effectively solve the problem we want to solve, e.g. determine if there is a cat on the image or not. (*From Deep Learning in Python (2nd edition) by Francois Chollet*). 
- Neural networks provide a generalisation of many other more traditional statistical methods. For example, the first layer of a strictly dense network looks a lot like a Principal Component Analysis. Therefore, if correctly configured a simple dense network might exactly mimic Princliple Component Regression (*no source*). 
- Neural networks seem like a magical machine that can solve problems by simply feeding it data. THis is however not the case, neural networks still take a lot of configuration and experimentation. The intuituin here is that in neural networks we do not solve the problem directly, but we build a (reprepsentation) machine that solves the problem for us. COnfiguring this machine (amount of layers, types of layers, etc) is still a task we need to perform, but the network can solve problems we deemded unsolvable just a few years ago. 
- Classification can be seen as drawing optimal boundaries between training samples in the hyperdimensional space spanned by the training features. Different types of regression can draw different types of boundaries, some purely straight, some in much more elaborate ways. Some methods first transform the space and then draw the lines, for example Principal Component Regression. The idea being that the classificaition boundaries can be drawn more efficiently in this newly transformed space. Approaches like deep learning take this transformation of space approach to the n-th degree, creating extremely complex manifolds to untangle the data into a representation that efficiently allows classification. Chollet likens this to folding open a piece of paper that has been 'frommeld op (kan de engelse term even niet bedenken)'. 
- In a deep neural net, different parts of the network specialise in solving a specific part of the ML problem. Some part might generate features, some choose features, some create embeddings, some classify the generated features for some task. Really good example are the encoder-decoder networks for images. You can take the encoder part and reuse it with a different classification backend. But now you can pretrain the encoder using self supervised learning. (Suggested by Rob Wanders). 
- Many neural network layers try to incororate the context around the direct local data to learn more effectively. For example, a recurrent network can predict the next work given the previous few words. Another example are CNN's, where a spatial kernel is used to create representation of the direct neighborhood surrounding a pixel. But taking into account much more contextual data comes at the price of computation times (?? is this even the case, or is the focus more and the second reason) and trainability. Each layer tries to balance taking into account contextual data without letting computation getting out of hand. For CNN's you can employ maxpooling layers to reduce the size of the data, LSTM's can learn which words are important, and transformers use attention. 
- Neural networks act like compression algorithms, where the information in the training data is cleverly combined in order to solve the problem you want to solve. This explains why a network needs to be of sufficient size in terms of coefficients, if the network is too small the compression is so much that the compressed data is no longer fit to solve the problem. 